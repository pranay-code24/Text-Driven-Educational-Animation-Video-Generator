#!/usr/bin/env python3
"""
GitHub Actions Video Renderer
Processes videos from Appwrite queue and uploads results back
"""

import os
import sys
import json
import asyncio
from typing import List, Optional
import warnings

# Suppress pkg_resources warning from manim_voiceover
warnings.filterwarnings("ignore", category=UserWarning, module="manim_voiceover.__init__")

# Add the project root to Python path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.query import Query
from datetime import datetime, timezone

from generate_video import VideoGenerator
from mllm_tools.litellm import LiteLLMWrapper
from src.core.appwrite_integration import AppwriteVideoManager
from src.config.config import Config

class GitHubVideoRenderer:
    """Handles video rendering in GitHub Actions environment"""
    
    def __init__(self):
        """Initialize the renderer with Appwrite connection"""
        self.client = Client()
        self.client.set_endpoint(os.getenv('APPWRITE_ENDPOINT'))
        self.client.set_project(os.getenv('APPWRITE_PROJECT_ID'))
        self.client.set_key(os.getenv('APPWRITE_API_KEY'))
        
        self.databases = Databases(self.client)
        self.appwrite_manager = AppwriteVideoManager()
        
    async def get_queued_videos(self) -> List[dict]:
        """Get videos that need rendering"""
        try:
            result = self.databases.list_documents(
                database_id="video_metadata",
                collection_id="videos",
                queries=[
                    Query.equal("status", ["queued_for_render", "ready_for_render"]),
                    Query.limit(5)  # Process 5 videos max per run
                ]
            )
            return result['documents']
        except Exception as e:
            print(f"Error fetching queued videos: {e}")
            return []
    
    async def update_video_status(self, video_id: str, status: str, error_message: str = None):
        """Update video status in database"""
        try:
            update_data = {
                "status": status,
                "updated_at": datetime.now(timezone.utc).isoformat()
            }
            if error_message:
                update_data["error_message"] = error_message
                
            self.databases.update_document(
                database_id="video_metadata",
                collection_id="videos",
                document_id=video_id,
                data=update_data
            )
            print(f"Updated video {video_id} status to: {status}")
        except Exception as e:
            print(f"Error updating video status: {e}")
    
    async def render_video(self, video_doc: dict) -> bool:
        """Render a single video"""
        video_id = video_doc['$id']
        topic = video_doc['topic']
        description = video_doc.get('description', '')
        
        print(f"Starting video rendering for: {topic} (ID: {video_id})")
        
        try:
            # Update status to rendering
            await self.update_video_status(video_id, "rendering")
            
            # Determine which LLM models to use for planning and scene generation
            planner_model_name = os.getenv('DEFAULT_PLANNER_MODEL', Config.DEFAULT_PLANNER_MODEL)
            scene_model_name = os.getenv('DEFAULT_SCENE_MODEL', Config.DEFAULT_SCENE_MODEL)
            helper_model_name = os.getenv('DEFAULT_HELPER_MODEL', Config.DEFAULT_HELPER_MODEL)
            
            # Get model parameters from environment or config
            model_temperature = float(os.getenv('DEFAULT_MODEL_TEMPERATURE', Config.DEFAULT_MODEL_TEMPERATURE))
            max_retries = int(os.getenv('DEFAULT_MAX_RETRIES', Config.DEFAULT_MAX_RETRIES))

            # Log the chosen models so they appear in the GitHub Actions output
            print("üß† Using planner model:", planner_model_name)
            print("üé¨ Using scene model:", scene_model_name)
            print("ü§ù Using helper model:", helper_model_name)
            print("üå°Ô∏è Model temperature:", model_temperature)
            print("üîÑ Max retries:", max_retries)

            # Initialize the LLM wrappers
            planner_model = LiteLLMWrapper(
                model_name=planner_model_name,
                temperature=model_temperature,
                print_cost=Config.MODEL_PRINT_COST,
                verbose=Config.MODEL_VERBOSE,
                use_langfuse=Config.USE_LANGFUSE
            )
            scene_model = LiteLLMWrapper(
                model_name=scene_model_name,
                temperature=model_temperature,
                print_cost=Config.MODEL_PRINT_COST,
                verbose=Config.MODEL_VERBOSE,
                use_langfuse=Config.USE_LANGFUSE
            )
            helper_model = LiteLLMWrapper(
                model_name=helper_model_name,
                temperature=model_temperature,
                print_cost=Config.MODEL_PRINT_COST,
                verbose=Config.MODEL_VERBOSE,
                use_langfuse=Config.USE_LANGFUSE
            )
            
            # Initialize video generator
            generator = VideoGenerator(
                planner_model=planner_model,
                scene_model=scene_model,
                helper_model=helper_model,
                output_dir=f"output/{video_id}",
                verbose=True,
                use_rag=Config.USE_RAG,
                use_context_learning=Config.USE_CONTEXT_LEARNING,
                use_visual_fix_code=Config.USE_VISUAL_FIX_CODE,
                use_appwrite=True
            )
            
            print(f"Generating video for topic: {topic}")
            
            # Generate the video using the pipeline
            await generator.generate_video_pipeline(
                topic=topic,
                description=description,
                max_retries=max_retries  # ‚úÖ Use configurable max retries
            )
            
            # Note: The VideoGenerator.generate_video_pipeline already updates the status to "completed"
            # with the combined_video_url, so we don't need to update it again here
            print(f"Successfully rendered video {video_id}")
            return True
            
        except Exception as e:
            error_message = str(e)
            print(f"‚ùå Error rendering video {video_id}: {error_message}")
            
            # Check if this is a scene max retry failure
            if "Max retries" in error_message and "reached for scene" in error_message:
                print(f"üõë Scene reached maximum retries - aborting entire video generation")
                await self.update_video_status(video_id, "failed", error_message)
                # Re-raise the exception to fail the GitHub Actions workflow
                raise Exception(f"Video generation aborted due to scene failure: {error_message}")
            else:
                # Handle other types of errors
                await self.update_video_status(video_id, "failed", error_message)
                return False
    
    async def process_queue(self):
        """Process all queued videos"""
        print("Checking for queued videos...")
        
        videos = await self.get_queued_videos()
        
        if not videos:
            print("No videos in queue")
            return
            
        print(f"Found {len(videos)} videos to process")
        
        success_count = 0
        for video in videos:
            try:
                success = await self.render_video(video)
                if success:
                    success_count += 1
            except Exception as e:
                # If the exception was raised due to scene max retries, re-raise it to fail the workflow
                if "Video generation aborted due to scene failure" in str(e):
                    print(f"üõë Aborting entire workflow due to scene failure: {e}")
                    raise e
                else:
                    print(f"‚ùå Failed to process video {video['$id']}: {e}")
        
        print(f"Processed {success_count}/{len(videos)} videos successfully")

async def main():
    """Main entry point"""
    renderer = GitHubVideoRenderer()
    
    # Check if specific video ID was provided
    video_id = os.getenv('VIDEO_ID')
    if video_id:
        print(f"Processing specific video: {video_id}")
        # Get the specific video document
        try:
            video_doc = renderer.databases.get_document(
                database_id="video_metadata",
                collection_id="videos", 
                document_id=video_id
            )
            await renderer.render_video(video_doc)
        except Exception as e:
            error_message = str(e)
            print(f"‚ùå Error processing video {video_id}: {error_message}")
            
            # If the exception was raised due to scene max retries, exit with error code
            if "Video generation aborted due to scene failure" in error_message:
                print(f"üõë Exiting with error code due to scene failure")
                sys.exit(1)
            else:
                print(f"‚ö†Ô∏è Video processing failed, but continuing workflow")
    else:
        # Process all queued videos
        try:
            await renderer.process_queue()
        except Exception as e:
            error_message = str(e)
            print(f"‚ùå Error processing video queue: {error_message}")
            
            # If the exception was raised due to scene max retries, exit with error code
            if "Video generation aborted due to scene failure" in error_message:
                print(f"üõë Exiting with error code due to scene failure")
                sys.exit(1)
            else:
                print(f"‚ö†Ô∏è Queue processing failed, but continuing workflow")

if __name__ == "__main__":
    asyncio.run(main()) 